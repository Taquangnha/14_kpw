import textwrap
import os
# import requests
from bs4 import BeautifulSoup
import re
import string
import nltk
from nltk.corpus import stopwords
import emot
from emot.emo_unicode import UNICODE_EMOJI # For emojis
from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS
input_folder_path = "C:\kpw\Data2"
output_folder_path = "C:\kpw\Data2\Data_Preprocessing"
if not os.path.exists(output_folder_path):
    os.makedirs(output_folder_path)

# Lấy danh sách các file trong thư mục gốc
files = os.listdir(input_folder_path)

# Duyệt qua danh sách các file và xử lý sau đó lưu vào từng file riêng trong thư mục "Data_Preprocessing"
for file in files:
    input_file_path = os.path.join(input_folder_path, file)
    output_file_path = os.path.join(output_folder_path, file)
    if os.path.isfile(input_file_path):
        with open(input_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            end_text = content.lower()
                # Loại bỏ thẻ html
            def remove_tags(html):
                    # parse html content
                    soup = BeautifulSoup(html, "html.parser")
                    for data in soup(['style', 'script']):
                        data.decompose() # Remove tags
                    return ' '.join(soup.stripped_strings)
            end_text=remove_tags(end_text)
                # Tách câu
            end_text=nltk.sent_tokenize(end_text)
            end_text=str(end_text)
                # # Xóa dấu câu
            for c in string.punctuation:
                    end_text= end_text.replace(c,'')
            end_text=end_text
                # Xóa chữ số
            end_text=str(end_text)
                # end_text = ' '.join(end_text)
            end_text = re.sub("\d+", " ", end_text)
                # Chuyển biểu tượng cảm xúc thành văn bản
            def converting_emojis(end_text):
                    for x in EMOTICONS_EMO:
                        end_text = end_text.replace(x, "_".join(EMOTICONS_EMO[x].replace(",","").replace(":","").split()))
                    for x in UNICODE_EMOJI:
                        end_text = end_text.replace(x, "_".join(UNICODE_EMOJI[x].replace(",","").replace(":","").split()))
                    return end_text
            end_text=converting_emojis(end_text)
                # Xóa các từ không có nghĩa
            stop = stopwords.words('english')
            end_text = " ".join(end_text for end_text in end_text.split() if end_text not in stop)
                # Chuẩn hóa văn bản
            lookup_dict = {'nlp':'natural language processing', 'ur':'your', "wbu" : "what about you"}
            def text_std(end_text):
                    words = end_text.split()
                    end_text=""
                    for word in words:
                        w=word
                        w = re.sub(r'[^\w\s]','',w)
                        if w.lower() in lookup_dict:
                            word=lookup_dict[w]
                        end_text=end_text + " " + word
                    return end_text
            end_text=text_std(end_text)
                # Sửa lỗi chính tả
            from autocorrect import spell
            end_text=spell(end_text)
                # Tách từ
            end_text=nltk.word_tokenize(end_text)
            for a in end_text:
                 
            # Sau đó lưu nội dung đã xử lý vào từng file riêng trong thư mục "Data_Preprocessing"
                with open(output_file_path, 'a', encoding='utf-8') as output_file:
                    output_file.write(a+" ")
                    with open("tonghopfile.txt", "a", encoding="utf-8") as file:
                    # Ghi nội dung vào tệp tin
                        file.write(a+", ")
