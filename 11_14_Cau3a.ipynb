import nltk
from nltk.corpus import stopwords
from nltk.probability import FreqDist
with open("tonghopfile.txt", "r", encoding="utf-8") as file:
    text=file.read()
text=nltk.word_tokenize(text)
from functools import reduce

# B1. Tach tu
# B2: Xay dung tu dien
from sklearn.feature_extraction.text import CountVectorizer
# Xay dung vector BOW
vect = CountVectorizer()
X = vect.fit_transform(text)

# Xay dung tu dien
dictionary=list(vect.get_feature_names_out())

print("Words in dictionary: ", dictionary)
print("Vector Bag-of-Word: \n", X.toarray())
