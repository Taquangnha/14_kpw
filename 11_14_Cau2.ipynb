from nltk.probability import FreqDist
with open("tonghopfile.txt", "r", encoding="utf-8") as file:
    text=file.read()
text=nltk.word_tokenize(text)
print("Number of words: ",len(text))
frequency_dist = FreqDist(word.lower() for word in text)

## show only th top 50 results
print(frequency_dist.most_common(50))

from nltk.probability import FreqDist
with open("tonghopfile.txt", "r", encoding="utf-8") as file:
    text=file.read()
text=nltk.word_tokenize(text)
print("Number of words: ",len(text))
frequency_dist = FreqDist(word.lower() for word in text)

## show only th top 50 results
print(frequency_dist.most_common(50))

large_words = dict([(k,v) for k,v in frequency_dist.items() if len(k)>3])
frequency_dist = nltk.FreqDist(large_words)
frequency_dist.plot(50,cumulative=False)


from wordcloud import WordCloud
wcloud = WordCloud().generate_from_frequencies(frequency_dist)
#plotting the wordcloud
import matplotlib.pyplot as plt
plt.imshow(wcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
